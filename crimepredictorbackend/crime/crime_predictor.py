# -*- coding: utf-8 -*-
"""crime-predictor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tkRoBWAAcgi1SvVJdpfiB39XJGaDuep2
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

# %matplotlib inline
sns.set_style("whitegrid")
plt.style.use("fivethirtyeight")

script_dir = os.path.dirname(__file__)
        # Load the model using the full path
df = pd.read_csv(os.path.join(script_dir,'meru_crime_df.csv'))

'''
import geopandas as gpd
from shapely.geometry import Point

gdf = df.copy()
gdf['Coordinates'] = gpd.points_from_xy(df['longitude'], df['latitude'])
gdf.Coordinates = gdf.Coordinates.apply(Point)
gdf = gpd.GeoDataFrame(gdf, geometry='Coordinates', crs={'init': 'epsg:4326'})

world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))
kenya = world[world.name == 'Kenya']
print(kenya.name)
# Plot Kenya
ax = kenya.plot(figsize=(14, 10), alpha=0.5, color='#d66058', zorder=1)
ax = gpd.GeoSeries(kenya['geometry'].unary_union).boundary.plot(ax=ax, alpha=0.5, color="#ed2518", zorder=1)

# Plot data points on the Kenyan map
gdf = gdf[gdf.Coordinates.apply(lambda geom: kenya.geometry.unary_union.contains(geom))]
gdf.plot(ax=ax, color='red')

plt.show()'''
'''
corr_matrix = df[['year' , 'proximity_to_bank' , 'latitude' , 'longitude' ]].corr()
plt.figure(figsize=(8, 12))

plt.title("Correlation Heatmap")

cmap = sns.diverging_palette( 1000, 120, as_cmap=True)
sns.heatmap(corr_matrix, annot=True, fmt='.2f',  linewidths=.8, cmap='coolwarm');

df['type'].value_counts()

from sklearn.preprocessing import StandardScaler

sdsc = StandardScaler()
feat_to_scale = ['year', 'month', 'day']
df[feat_to_scale] = sdsc.fit_transform(df[feat_to_scale])
df.head(2)'''

from sklearn.preprocessing import LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin

class EncodeCategories(BaseEstimator, TransformerMixin):
    """
    This class encodes categories in the specified column into numerical values.
    """
    def __init__(self, column):
        self.column = column
        self.encoder = LabelEncoder()

    def fit(self, X, y=None):
        # Fit the LabelEncoder on the specified column
        self.encoder.fit(X[self.column])
        return self



    def transform(self, X):
        # Transform the specified column using LabelEncoder
        X[self.column] = self.encoder.transform(X[self.column])
        return X

from sklearn.preprocessing import LabelEncoder

class DecodeCrimeType(BaseEstimator, TransformerMixin):
    """
    This class decodes numerical values in the specified column into their original categorical values.
    """
    def __init__(self, column):
        self.column = column
        self.encoder = LabelEncoder(handle_unknown='ignore')

    def fit(self, X, y=None):
        # Fit the LabelEncoder on the specified column
        self.encoder.fit(X[self.column])
        return self




    def transform(self, X):
        # Transform the specified column using LabelEncoder
        X[self.column] = self.encoder.inverse_transform(X[self.column])
        return X
from sklearn.pipeline import Pipeline

pipeline = Pipeline([("LabelEncoder" , EncodeCategories(column='type')) ])

df = pipeline.fit_transform(df)
df.head()

from sklearn.model_selection import train_test_split
features = ['year' , 'month' , 'day' , 'latitude' , 'longitude' , 'type']

# Split the data into training and testing sets
X = df[features].drop(['type'], axis=1)
y = df['type']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
# from sklearn.preprocessing import StandardScaler


# sc = StandardScaler()
# # X_train = scaler.fit_transform(X)
# # y_train = y.to_numpy()
# X_train = sc.fit_transform(X_train)
# X_test = sc.transform(X_test)


# X_train
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV


train_classifier = RandomForestClassifier()

param  = [

    {"n_estimators":[10 ,100,200,400] , "max_depth":[None , 5, 10] , "min_samples_split":[2,3,4]}

]

grid_search = GridSearchCV(train_classifier , param , cv=3 , scoring="accuracy" , return_train_score=True)
grid_search.fit(X_train , y_train)

model = grid_search.best_estimator_
model

# test_df = pipeline.fit_transform(test_df)
# test_df = test_df.copy()
# # test_df.fillna(method='ffill' , inplace=True)
# scaler = StandardScaler()
# X_test = scaler.fit_transform(test_df)

preds =  model.predict(X_test)

preds

from sklearn.metrics import accuracy_score

# Make predictions on the test set
preds = model.predict(X_test)

# Calculate the accuracy score
accuracy = accuracy_score(y_test, preds)
print(f"Model Accuracy: {accuracy:.4f}")

feature_importances = model.feature_importances_

import joblib
joblib.dump(model, 'random_forest_model.pkl')

result_df = pd.DataFrame({
    'year': X_test['year'],
    'month': X_test['month'],
    'day': X_test['day'],
    'latitude': X_test['latitude'],
    'longitude': X_test['longitude'],
    'true_type': y_test,
    'predicted_type': preds,
    'feature_importances': [feature_importances] * len(X_test)
})

# Save the DataFrame to a CSV file
result_df.to_csv('predictions.csv', index=False)

result_df.head()

# pipeline = Pipeline([("DecodeCrimeType" , DecodeCrimeType(column='predicted_type')) ])

# result_df = pipeline.fit_transform(df)
# df.head()